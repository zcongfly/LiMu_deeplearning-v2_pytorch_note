{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56e5abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de818d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本编码器接口\"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super(Encoder,self).__init__(**kwargs)\n",
    "        \n",
    "    def forward(self,X,*args):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f92b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基本解码器接口\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba17f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"编码器-解码器架构的基类\"\"\"\n",
    "    def __init__(self,encoder,decoder,**kwargs):\n",
    "        super(EncoderDecoder,self).__init__(**kwargs)\n",
    "        self.decoder=decoder\n",
    "        self.encoder=encoder\n",
    "        \n",
    "    def forward(self,enc_X,dec_X,*args):\n",
    "        enc_outputs=self.encoder(enc_X,*args)\n",
    "        dec_state=self.decoder.init_state(enc_outputs,*args)\n",
    "        return self.decoder(dec_X,dec_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e8847",
   "metadata": {},
   "source": [
    "从技术上讲，编码器将长度可变的出入序列转换成形状固定的上下文变量c，并且将输入序列的信息在该上下文变量中进行编码。\n",
    "可以使用循环神经网络来设置编码器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e6ef47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(Encoder):\n",
    "    \"\"\"用于序列到序列的循环神经网络\"\"\"\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)# 嵌入层\n",
    "        self.gru=nn.GRU(embed_size,num_hiddens,num_layers,dropout=dropout)# GRU层\n",
    "        \n",
    "    def forward(self,X,*args):\n",
    "        X=self.embedding(X)#(batch_size,num_steps,embed_size)\n",
    "        # 在循环神经网络模型中，第一个轴对应于时间步\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # 如果未提及状态，则默认为0\n",
    "        output, state = self.gru(X)\n",
    "        # output的形状:(num_steps,batch_size,num_hiddens)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42a1f8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([7, 4, 16]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "encoder.eval()\n",
    "X = torch.zeros((4, 7), dtype=torch.long)\n",
    "output, state = encoder(X)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7f2241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    \"\"\"用于序列到序列学习的循环神经网络解码器\"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.gru = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers,\n",
    "                          dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "\n",
    "    def forward(self, X, state):\n",
    "        # 输出'X'的形状：(batch_size,num_steps,embed_size)\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        output, state = self.gru(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output的形状:(batch_size,num_steps,vocab_size)\n",
    "        # state的形状:(num_layers,batch_size,num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7b742",
   "metadata": {},
   "source": [
    "```python\n",
    "context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "X_and_context = torch.cat((X, context), 2)\n",
    "```\n",
    "在Seq2Seq解码器中，context是编码器的最终隐藏状态。它用于将当前时间步的解码器输入与先前的上下文信息结合起来，以便更好地预测下一个时步的输出。具体地说，上下文通过广播（repeat）和连接（concatenation）操作与当前时间步的解码器输入一起馈送到GRU神经元中进行处理。这允许模型同时考虑先前的上下文信息和当前的输入信息，从而产生更准确的输出预测。\n",
    "\n",
    "先前的上下文信息指的是之前时间步的编码器最终隐藏状态（也称为编码器状态或编码器输出）。在Seq2Seq模型中，编码器将输入序列转换成一个固定长度的向量作为上下文信息，然后解码器使用这个向量作为先前的上下文信息来帮助预测当前时间步的输出。因此，解码器的输入不仅包括当前时间步的输入，还包括先前的上下文信息（通常通过广播和连接操作得到）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c8620ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                         num_layers=2)\n",
    "decoder.eval()\n",
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd447732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
