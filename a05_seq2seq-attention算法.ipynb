{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.functional import cross_entropy, softmax\n",
    "from torch.utils.data import DataLoader\n",
    "import datetime\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese time order: yy/mm/dd  ['31-04-26', '04-07-18', '33-06-06'] \n",
      "English time order: dd/M/yyyy ['26/Apr/2031', '18/Jul/2004', '06/Jun/2033']\n",
      "Vocabularies:  {'Jul', 'Feb', '4', 'Dec', '<GO>', '7', 'Oct', '2', 'Aug', 'Mar', '<EOS>', '1', '/', '3', 'Nov', 'Apr', 'Jan', '8', '0', 'Jun', '<PAD>', '-', '9', '6', 'May', '5', 'Sep'}\n",
      "x index sample:  \n",
      "31-04-26\n",
      "[6 4 1 3 7 1 5 9] \n",
      "y index sample:  \n",
      "<GO>26/Apr/2031<EOS>\n",
      "[14  5  9  2 15  2  5  3  6  4 13]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "hidden0 has inconsistent hidden_size: got 1, expected 32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 153\u001B[0m\n\u001B[0;32m    143\u001B[0m                 \u001B[38;5;28mprint\u001B[39m(\n\u001B[0;32m    144\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch: \u001B[39m\u001B[38;5;124m\"\u001B[39m,i,\n\u001B[0;32m    145\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m| t: \u001B[39m\u001B[38;5;124m\"\u001B[39m, batch_idx,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    149\u001B[0m                     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m| inference: \u001B[39m\u001B[38;5;124m\"\u001B[39m, res,\n\u001B[0;32m    150\u001B[0m                 )\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 153\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[7], line 140\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_idx \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m70\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    139\u001B[0m     target \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39midx2str(by[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[1;32m--> 140\u001B[0m     pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbx\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    141\u001B[0m     res \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39midx2str(pred[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[0;32m    142\u001B[0m     src \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39midx2str(bx[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mnumpy())\n",
      "Cell \u001B[1;32mIn[7], line 71\u001B[0m, in \u001B[0;36mSeq2Seq.inference\u001B[1;34m(self, x, return_align)\u001B[0m\n\u001B[0;32m     65\u001B[0m context \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(att_weight,o)    \u001B[38;5;66;03m# [n, 1, units]\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# attn_prod = torch.matmul(self.attn(o),hx.unsqueeze(2))  # [n, step, 1]\u001B[39;00m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# attn_weight = softmax(attn_prod,dim=1)                  # [n, step, 1]\u001B[39;00m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;66;03m# context = torch.matmul(o.permute(0,2,1),attn_weight)    # [n, units, 1]\u001B[39;00m\n\u001B[0;32m     69\u001B[0m \n\u001B[0;32m     70\u001B[0m \u001B[38;5;66;03m# 更新隐藏状态(hx, cx)\u001B[39;00m\n\u001B[1;32m---> 71\u001B[0m hx, cx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder_cell\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdec_in\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;66;03m# 将上下文向量context和更新后的隐藏状态hx在维度1上进行拼接\u001B[39;00m\n\u001B[0;32m     73\u001B[0m hc \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([context\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m),hx],dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)           \u001B[38;5;66;03m# [n, units *2]\u001B[39;00m\n",
      "File \u001B[1;32mC:\\self_download\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mC:\\self_download\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1237\u001B[0m, in \u001B[0;36mLSTMCell.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m   1234\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1235\u001B[0m     hx \u001B[38;5;241m=\u001B[39m (hx[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m), hx[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched \u001B[38;5;28;01melse\u001B[39;00m hx\n\u001B[1;32m-> 1237\u001B[0m ret \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm_cell\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1238\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1239\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight_ih\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight_hh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1240\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_ih\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_hh\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1241\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_batched:\n\u001B[0;32m   1244\u001B[0m     ret \u001B[38;5;241m=\u001B[39m (ret[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m), ret[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m0\u001B[39m))\n",
      "\u001B[1;31mRuntimeError\u001B[0m: hidden0 has inconsistent hidden_size: got 1, expected 32"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    # enc_v_dim:输入编码器的文本的样本数（输入序列的词汇表大小）\n",
    "    # dec_v_dim:输入解码器的文本样本数\n",
    "    # emb_dim:嵌入层向量维度\n",
    "    # units:lstm层中隐藏层单元的数量\n",
    "    # max_pred_len:最大预测长度\n",
    "    # start_token:开始标记\n",
    "    # end_token:结束标记\n",
    "    def __init__(self,enc_v_dim,dec_v_dim,emb_dim,units,max_pred_len,start_token,end_token):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.units=units\n",
    "        self.dec_v_dim=dec_v_dim\n",
    "\n",
    "        # encoder\n",
    "        self.enc_embeddings=nn.Embedding(enc_v_dim,emb_dim) #编码器嵌入层\n",
    "        self.enc_embeddings.weight.data.normal_(0,0.1)      #初始化嵌入层权重\n",
    "        self.encoder = nn.LSTM(emb_dim,units,1,batch_first=True)    #LSTM层\n",
    "\n",
    "        # decoder\n",
    "        self.dec_embeddings = nn.Embedding(dec_v_dim,emb_dim)    #解码器嵌入层\n",
    "        self.attn=nn.Linear(units,units)\n",
    "        self.decoder_cell=nn.LSTMCell(emb_dim,units)\n",
    "        self.decoder_dense = nn.Linear(units*2,dec_v_dim)\n",
    "\n",
    "        self.opt=torch.optim.Adam(self.parameters(),lr=0.001)   #Adam优化器\n",
    "        self.max_pred_len=max_pred_len  #最大预测长度\n",
    "        self.start_token=start_token    #开始标记\n",
    "        self.end_token=end_token        #结束标记\n",
    "\n",
    "    # 编码，这个函数作为带训练文本输入的入口\n",
    "    def encode(self,x):\n",
    "        embedded = self.enc_embeddings(x)   #将x作为参数传入编码器的嵌入层，x对应的输出嵌入向量，emb.shape=[n,step,emb](样本数，时间步，嵌入向量维度)\n",
    "        hidden = (torch.zeros(1,x.shape[0],self.units),torch.zeros(1,x.shape[0],self.units))    #LSTM的初始状态，h表示lstm的初始隐藏状态，c表示lstm的初始细胞状态([1,n,units],[1,n,units)\n",
    "        o,(h,c)=self.encoder(embedded,hidden)   #将嵌入向量和初始状态喂入lstm\n",
    "        return o,h,c    #返回输出数据和新的隐藏状态和细胞状态\n",
    "\n",
    "    # 基于注意力机制的解码过程\n",
    "    def inference(self,x,return_align=False):\n",
    "        self.eval() #开启评估模式\n",
    "        o,hx,cx=self.encode(x)  #将x传入编码器，返回输出o和新的隐藏状态hx以及新的初始化状态cx，维度[n, step, units], [num_layers * num_directions, n, units] * 2\n",
    "        \"\"\"\n",
    "        对于单层单向的 LSTM，`hx`或`cx`的第一维度是1，表示只有一个层。`num_layers`表示 LSTM 层的数量，对于单层 LSTM，`num_layers`为1。而`num_directions`表示 LSTM 的方向数，有两种情况：\n",
    "            - 单向 LSTM：`num_directions`为1，表示只有一个方向。\n",
    "            - 双向 LSTM：`num_directions`为2，表示正向和反向两个方向。\n",
    "        在双向 LSTM 中，隐藏状态和细胞状态的第一维度会乘以`num_directions`，以保存每个方向的状态。所以如果是多层的双向 LSTM，`hx`和`cx`的第一维度可以表示为`num_layers * num_directions`。\n",
    "        \"\"\"\n",
    "\n",
    "        #创建开始标记\n",
    "        start = torch.ones(x.shape[0],1)    # [n, 1]\n",
    "        start[:,0] = torch.tensor(self.start_token)\n",
    "        start= start.type(torch.LongTensor)\n",
    "        # 将开始标记转换成嵌入向量作为解码器的输入\n",
    "        dec_emb_in = self.dec_embeddings(start) # [n, 1, emb_dim]\n",
    "        dec_emb_in = dec_emb_in.permute(1,0,2)  # [1, n, emb_dim]\n",
    "        dec_in = dec_emb_in[0]                  # [n, emb_dim]\n",
    "\n",
    "        output = []\n",
    "        for i in range(self.max_pred_len):\n",
    "            # 计算注意力权重\n",
    "            attn_prod = torch.matmul(self.attn(hx.unsqueeze(1)),o.permute(0,2,1)) # 将编码器的隐藏状态hx在维度1上扩展，与解码器的输出o进行点积运算，得到注意力分数attn_prod[n, 1, step]\n",
    "            att_weight = softmax(attn_prod, dim=2)  # 得到注意力权重att_weight[n, 1, step]\n",
    "\n",
    "            # 计算上下文向量context\n",
    "            # 上下文向量（Context Vector）是在注意力机制中用于表示编码器输出在解码器中的加权汇总信息。它起到将编码器的输出与当前解码器的隐藏状态相结合的作用，以便更好地捕捉相关信息。\n",
    "            context = torch.matmul(att_weight,o)    # [n, 1, units]\n",
    "            # attn_prod = torch.matmul(self.attn(o),hx.unsqueeze(2))  # [n, step, 1]\n",
    "            # attn_weight = softmax(attn_prod,dim=1)                  # [n, step, 1]\n",
    "            # context = torch.matmul(o.permute(0,2,1),attn_weight)    # [n, units, 1]\n",
    "\n",
    "            # 更新隐藏状态(hx, cx)\n",
    "            hx, cx = self.decoder_cell(dec_in, (hx, cx))\n",
    "            # 将上下文向量context和更新后的隐藏状态hx在维度1上进行拼接\n",
    "            hc = torch.cat([context.squeeze(1),hx],dim=1)           # [n, units *2]\n",
    "            # hc = torch.cat([context.squeeze(2),hx],dim=1)           # [n, units *2]\n",
    "\n",
    "            # 输出结果\n",
    "            result = self.decoder_dense(hc) #将特征向量hc通过线性层decoder_dense映射为输出空间\n",
    "            result = result.argmax(dim=1).view(-1,1)    #取得概率最大的输出值作为当前时间步的输出\n",
    "            # 将当前时间步的输出作为解码器的嵌入输入dec_in，进入下一个时间步的循环\n",
    "            dec_in=self.dec_embeddings(result).permute(1,0,2)[0]\n",
    "            output.append(result)\n",
    "        output = torch.stack(output,dim=0)  #将输出结果堆叠到output中\n",
    "        self.train()    #将模型设为训练模式\n",
    "\n",
    "    # 实现了训练时的模型前向传播过程，包括了编码器的处理、注意力计算、解码器的处理和输出\n",
    "    def train_logit(self,x,y):\n",
    "        o,hx,cx = self.encode(x)    # 将x输入编码器，返回编码器输出o和隐藏状态hx/cx[n, step, units], [num_layers * num_directions, n, units] * 2\n",
    "        hx,cx = hx[0],cx[0]         # 获取编码器的最终隐藏状态[n, units]\n",
    "        dec_in = y[:,:-1]           # 从目标序列y中移除最后一个时间步，得到解码器的输入序列[n, step]\n",
    "        dec_emb_in = self.dec_embeddings(dec_in)    # [n, step, emb_dim]\n",
    "        dec_emb_in = dec_emb_in.permute(1,0,2)      # [step, n, emb_dim]\n",
    "        output = []\n",
    "        for i in range(dec_emb_in.shape[0]):\n",
    "            # General Attention:\n",
    "            # score(ht,hs) = (ht^T)(Wa)hs\n",
    "            # hs is the output from encoder\n",
    "            # ht is the previous hidden state from decoder\n",
    "            # self.attn(o): [n, step, units]\n",
    "            attn_prod = torch.matmul(self.attn(hx.unsqueeze(1)),o.permute(0,2,1)) # 计算注意力分数[n, 1, step]\n",
    "            att_weight = softmax(attn_prod, dim=2)  # 得到注意力权重[n, 1, step]\n",
    "            context = torch.matmul(att_weight,o)    # 计算上下文向量[n, 1, units]\n",
    "            # attn_prod = torch.matmul(self.attn(o),hx.unsqueeze(2))  # [n, step, 1]\n",
    "            # attn_weight = softmax(attn_prod,dim=1)                  # [n, step, 1]\n",
    "            # context = torch.matmul(o.permute(0,2,1),attn_weight)    # [n, units, 1]\n",
    "            hx, cx = self.decoder_cell(dec_emb_in[i], (hx, cx))     # [n, units]\n",
    "            hc = torch.cat([context.squeeze(1),hx],dim=1)           # [n, units *2]\n",
    "            # hc = torch.cat([context.squeeze(2),hx],dim=1)           # [n, units *2]\n",
    "            result = self.decoder_dense(hc)                              # [n, dec_v_dim]\n",
    "            output.append(result)\n",
    "        output = torch.stack(output,dim=0)  # [step, n, dec_v_dim]\n",
    "        return output.permute(1,0,2)        # [n, step, dec_v_dim]\n",
    "\n",
    "    # 完成了一次训练的过程，包括了前向传播、损失计算、反向传播和参数更新\n",
    "    def step(self,x,y):\n",
    "        self.opt.zero_grad()    #将梯度清零（以免以前的梯度干扰后续的代码执行）\n",
    "        batch_size = x.shape[0] #批次大小\n",
    "        logit = self.train_logit(x,y)   #前向传播，输出预测值\n",
    "        dec_out = y[:,1:]   #从目标序列 y 中移除第一个时间步，得到解码器的输出序列\n",
    "        loss = cross_entropy(logit.reshape(-1,self.dec_v_dim),dec_out.reshape(-1))#计算交叉熵损失，将模型输出和目标序列进行比较并计算损失值\n",
    "        loss.backward() #反向传播，计算模型参数的梯度\n",
    "        self.opt.step() #根据优化器的更新规则，更新模型参数\n",
    "        return loss.detach().numpy()    #返回损失值的 numpy 数组形式\n",
    "\n",
    "\n",
    "def train():\n",
    "    dataset = utils.DateData(4000)\n",
    "    print(\"Chinese time order: yy/mm/dd \",dataset.date_cn[:3],\"\\nEnglish time order: dd/M/yyyy\", dataset.date_en[:3])\n",
    "    print(\"Vocabularies: \", dataset.vocab)\n",
    "    print(f\"x index sample:  \\n{dataset.idx2str(dataset.x[0])}\\n{dataset.x[0]}\",\n",
    "    f\"\\ny index sample:  \\n{dataset.idx2str(dataset.y[0])}\\n{dataset.y[0]}\")\n",
    "    loader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
    "    model = Seq2Seq(dataset.num_word,dataset.num_word,emb_dim=16,units=32,max_pred_len=11,start_token=dataset.start_token,end_token=dataset.end_token)\n",
    "    for i in range(100):\n",
    "        for batch_idx , batch in enumerate(loader):\n",
    "            bx, by, decoder_len = batch\n",
    "            bx = bx.type(torch.LongTensor)  #将数据格式转换为LongTensor类型\n",
    "            by = by.type(torch.LongTensor)\n",
    "            loss = model.step(bx,by)\n",
    "            if batch_idx % 70 == 0:\n",
    "                target = dataset.idx2str(by[0, 1:-1].data.numpy())\n",
    "                pred = model.inference(bx[0:1])\n",
    "                res = dataset.idx2str(pred[0].data.numpy())\n",
    "                src = dataset.idx2str(bx[0].data.numpy())\n",
    "                print(\n",
    "                    \"Epoch: \",i,\n",
    "                    \"| t: \", batch_idx,\n",
    "                    \"| loss: %.3f\" % loss,\n",
    "                    \"| input: \", src,\n",
    "                    \"| target: \", target,\n",
    "                    \"| inference: \", res,\n",
    "                )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 细胞状态\n",
    "在循环神经网络（RNN）中，细胞状态（cell state）是一种记忆机制，用于在不同时间步之间传递和存储信息。\n",
    "\n",
    "在长短期记忆网络（LSTM）中，细胞状态是 LSTM 的核心组成部分。它可以看作是 LSTM 网络的长期记忆，用于捕捉输入序列中的相关信息并将其传递给后续时间步。细胞状态在 LSTM 中通过门控机制进行更新和调节，从而控制信息的流动和遗忘。\n",
    "\n",
    "细胞状态在 LSTM 中的更新是通过遗忘门（forget gate）、输入门（input gate）和输出门（output gate）来实现的。遗忘门决定是否保留之前的细胞状态信息，输入门决定更新的新信息，输出门决定将哪些信息传递给下一个时间步。\n",
    "\n",
    "细胞状态的设计使得 LSTM 能够在处理长序列数据时更好地捕捉长期依赖关系，并且在训练过程中可以通过反向传播进行梯度更新。通过使用细胞状态，LSTM 能够有效地处理序列数据，例如自然语言处理、语音识别等任务。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}