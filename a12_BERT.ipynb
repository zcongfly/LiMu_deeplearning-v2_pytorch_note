{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import cross_entropy,softmax, relu\n",
    "\n",
    "import utils\n",
    "from GPT import GPT\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "MASK_RATE=0.15"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class BERT(GPT):\n",
    "    def __init__(self,model_dim,max_len,num_layer,num_head,n_vocab,lr,\n",
    "                 max_seg=3,drop_rate=0.2,padding_idx=0) -> None:\n",
    "        # 在 super() 函数中调用父类的 __init__ 方法，传递相应的参数。这样可以初始化继承自父类的属性和执行父类中的初始化逻辑\n",
    "        super(BERT, self).__init__(model_dim,max_len,num_layer,num_head,n_vocab,lr,max_seg,drop_rate,padding_idx)\n",
    "\n",
    "    def step(self, seqs, segs, seqs_, loss_mask, nsp_labels):\n",
    "        \"\"\"\n",
    "        执行一次前向传播、计算损失、反向传播和参数更新的过程\n",
    "        :param seqs: 输入序列\n",
    "        :param segs: 分段序列\n",
    "        :param seqs_: 目标序列\n",
    "        :param loss_mask: 损失掩码\n",
    "        :param nsp_labels: 下一个句预测标签\n",
    "        :return: 总损失loss和MLM预测结果\n",
    "        \"\"\"\n",
    "        device=next(self.parameters()).device\n",
    "        self.opt.zero_grad()\n",
    "        mlm_logits,nsp_logits=self(seqs,segs,training=True) # 调用forward函数执行前向传播\n",
    "        mlm_loss=cross_entropy(\n",
    "            # 计算mlm损失，使用torch.masked_select根据损失掩码loss_mask选择有效位置的预测结果和目标序列进行交叉熵计算\n",
    "            torch.masked_select(mlm_logits,loss_mask).reshape(-1,mlm_logits.shape[2]),\n",
    "            torch.masked_select(seqs_,loss_mask.squeeze(2))\n",
    "        )\n",
    "        # 计算nsp损失\n",
    "        nsp_loss=cross_entropy(nsp_logits,nsp_labels.reshape(-1))\n",
    "        loss=mlm_loss+0.2*nsp_loss\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        return loss.cpu().data.numpy(),mlm_logits\n",
    "\n",
    "    def mask(self, seqs):\n",
    "        # 生成序列的掩码矩阵，以标识序列中的填充位置\n",
    "        # 通过比较序列中的元素与填充索引 self.padding_idx 是否相等，生成一个布尔类型的掩码矩阵\n",
    "        # 返回的掩码矩阵形状为 [batch_size, 1, 1, seq_len]，其中 batch_size 是序列的批大小，seq_len 是序列的长度。生成的掩码矩阵会在后续的注意力计算中使用，用于屏蔽填充位置的影响。\n",
    "        mask = torch.eq(seqs,self.padding_idx)\n",
    "        return mask[:, None, None, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# torch.masked_select()\n",
    "\n",
    "`torch.masked_select` 是一个用于按照指定掩码从张量中选择元素的函数。它的作用是根据给定的掩码，在输入张量中选择满足条件的元素。\n",
    "\n",
    "具体来说，对于第一行代码 `torch.masked_select(mlm_logits, loss_mask).reshape(-1, mlm_logits.shape[2])`：\n",
    "\n",
    "- `mlm_logits` 是 MLM 模型的预测结果，它是一个形状为 `[batch_size, seq_len, n_vocab]` 的张量，表示每个位置对应每个词的预测概率。\n",
    "- `loss_mask` 是损失掩码，它是一个形状为 `[batch_size, 1, seq_len, seq_len]` 的布尔类型张量，用于标识填充位置。\n",
    "- `torch.masked_select(mlm_logits, loss_mask)` 使用 `loss_mask` 对 `mlm_logits` 进行掩码选择，即只选择掩码为 True 的位置对应的元素。\n",
    "- `reshape(-1, mlm_logits.shape[2])` 将选择的元素进行形状重塑，将第二维的大小调整为 `mlm_logits.shape[2]`，也就是词汇表大小，以适应后续计算交叉熵损失的需要。\n",
    "\n",
    "对于第二行代码 `torch.masked_select(seqs_, loss_mask.squeeze(2))`：\n",
    "\n",
    "- `seqs_` 是目标序列，形状为 `[batch_size, seq_len]`，表示每个位置的真实词索引。\n",
    "- `loss_mask.squeeze(2)` 对 `loss_mask` 进行压缩操作，将第三维的大小为 1 的维度压缩去除，得到形状为 `[batch_size, 1, seq_len]` 的张量。\n",
    "- `torch.masked_select(seqs_, loss_mask.squeeze(2))` 使用 `loss_mask.squeeze(2)` 对 `seqs_` 进行掩码选择，即只选择掩码为 True 的位置对应的目标词索引。\n",
    "\n",
    "这两行代码的目的是根据掩码选择模型的预测结果和目标序列，以便后续计算 MLM 的交叉熵损失。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def _get_loss_mask(len_arange, seq, pad_id):\n",
    "    \"\"\"生成用于掩码语言建模任务的掩码数组,生成的掩码数组可以用于在训练过程中计算 MLM 的损失函数\"\"\"\n",
    "    rand_id = np.random.choice(len_arange, size=max(2, int(MASK_RATE * len(len_arange))), replace=False)    #从序列长度范围 len_arange 中随机选择一些索引作为要进行掩码的位置\n",
    "    loss_mask = np.full_like(seq, pad_id, dtype=np.bool)    #创建一个与输入序列 seq 形状相同的数组 loss_mask，并将其填充为布尔类型的 pad_id\n",
    "    loss_mask[rand_id] = True   #将选择的掩码位置在 loss_mask 中标记为 True，表示这些位置要进行掩码\n",
    "    return loss_mask[None, :], rand_id  #返回形状为 [1, seq_len] 的掩码数组 loss_mask，以及选择的掩码位置索引 rand_id\n",
    "\n",
    "def do_mask(seq, len_arange, pad_id, mask_id):\n",
    "    \"\"\"直接用特定的mask_id来mask掩码的位置\"\"\"\n",
    "    loss_mask, rand_id = _get_loss_mask(len_arange, seq, pad_id)\n",
    "    seq[rand_id] = mask_id\n",
    "    return loss_mask\n",
    "\n",
    "def do_replace(seq, len_arange, pad_id, word_ids):\n",
    "    \"\"\"在输入序列 seq 中随机替换一些位置的单词或标记\"\"\"\n",
    "    loss_mask, rand_id = _get_loss_mask(len_arange, seq, pad_id)    #生成掩码数组 loss_mask，用于指示要进行替换的位置\n",
    "    seq[rand_id] = torch.from_numpy(np.random.choice(word_ids, size=len(rand_id))).type(torch.IntTensor)    #从给定的 word_ids 中随机选择与掩码位置数目相同的单词或标记，并将其替换到输入序列中对应的位置上\n",
    "    return loss_mask\n",
    "\n",
    "def do_nothing(seq, len_arange, pad_id):\n",
    "    loss_mask, _ = _get_loss_mask(len_arange, seq, pad_id)\n",
    "    return loss_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def random_mask_or_replace(data,arange,dataset):\n",
    "    \"\"\"在训练过程中对输入数据进行随机的掩码、保持不变或替换操作，增加数据的多样性和模型的泛化能力\"\"\"\n",
    "    seqs, segs,xlen,nsp_labels = data\n",
    "    seqs_ = seqs.data.clone()\n",
    "    p = np.random.random()\n",
    "    if p < 0.7:\n",
    "        # mask\n",
    "        loss_mask = np.concatenate([\n",
    "            do_mask(\n",
    "                seqs[i],\n",
    "                np.concatenate((arange[:xlen[i,0]],arange[xlen[i,0]+1:xlen[i].sum()+1])),\n",
    "                dataset.pad_id,\n",
    "                dataset.mask_id\n",
    "                )\n",
    "                for i in range(len(seqs))], axis=0)\n",
    "    elif p < 0.85:\n",
    "        # do nothing\n",
    "        loss_mask = np.concatenate([\n",
    "            do_nothing(\n",
    "                seqs[i],\n",
    "                np.concatenate((arange[:xlen[i,0]],arange[xlen[i,0]+1:xlen[i].sum()+1])),\n",
    "                dataset.pad_id\n",
    "                )\n",
    "                for i in range(len(seqs))],  axis=0)\n",
    "    else:\n",
    "        # replace\n",
    "        loss_mask = np.concatenate([\n",
    "            do_replace(\n",
    "                seqs[i],\n",
    "                np.concatenate((arange[:xlen[i,0]],arange[xlen[i,0]+1:xlen[i].sum()+1])),\n",
    "                dataset.pad_id,\n",
    "                dataset.word_ids\n",
    "                )\n",
    "                for i in range(len(seqs))],  axis=0)\n",
    "    loss_mask = torch.from_numpy(loss_mask).unsqueeze(2)\n",
    "    return seqs, segs, seqs_, loss_mask, xlen, nsp_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def export_attention(model,device,data,name=\"bert\"):\n",
    "    model.load_state_dict(torch.load(\"./visual/models/bert/model.pth\",map_location=device))\n",
    "    seqs, segs,xlen,nsp_labels = data[:32]\n",
    "    seqs, segs,xlen,nsp_labels = torch.from_numpy(seqs),torch.from_numpy(segs),torch.from_numpy(xlen),torch.from_numpy(nsp_labels)\n",
    "    seqs, segs,nsp_labels = seqs.type(torch.LongTensor).to(device), segs.type(torch.LongTensor).to(device),nsp_labels.to(device)\n",
    "    model(seqs,segs,False)\n",
    "    seqs = seqs.cpu().data.numpy()\n",
    "    data = {\"src\": [[data.i2v[i] for i in seqs[j]] for j in range(len(seqs))], \"attentions\": model.attentions}\n",
    "    path = \"./visual/tmp/%s_attention_matrix.pkl\" % name\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train():\n",
    "    MODEL_DIM = 256\n",
    "    N_LAYER = 4\n",
    "    LEARNING_RATE = 1e-4\n",
    "    dataset = utils.MRPCData(\"./MRPC\",2000)\n",
    "    print(\"num word: \",dataset.num_word)\n",
    "    model = BERT(\n",
    "        model_dim=MODEL_DIM, max_len=dataset.max_len, num_layer=N_LAYER, num_head=4, n_vocab=dataset.num_word,\n",
    "        lr=LEARNING_RATE, max_seg=dataset.num_seg, drop_rate=0.2, padding_idx=dataset.pad_id\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU train avaliable\")\n",
    "        device =torch.device(\"cuda\")\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        model = model.cpu()\n",
    "\n",
    "    loader = DataLoader(dataset,batch_size=32,shuffle=True)\n",
    "    arange = np.arange(0,dataset.max_len)\n",
    "    for epoch in range(500):\n",
    "        for batch_idx, batch in enumerate(loader):\n",
    "            seqs, segs, seqs_, loss_mask, xlen, nsp_labels = random_mask_or_replace(batch,arange,dataset)\n",
    "            seqs, segs, seqs_, nsp_labels, loss_mask = seqs.type(torch.LongTensor).to(device), segs.type(torch.LongTensor).to(device),seqs_.type(torch.LongTensor).to(device),nsp_labels.to(device),loss_mask.to(device)\n",
    "            loss, pred = model.step(seqs, segs, seqs_, loss_mask, nsp_labels)\n",
    "            if batch_idx % 100 == 0:\n",
    "                pred = pred[0].cpu().data.numpy().argmax(axis=1)\n",
    "                print(\n",
    "                \"\\n\\nEpoch: \",epoch,\n",
    "                \"|batch: \", batch_idx,\n",
    "                \"| loss: %.3f\" % loss,\n",
    "                \"\\n| tgt: \", \" \".join([dataset.i2v[i] for i in seqs[0].cpu().data.numpy()[:xlen[0].sum()+1]]),\n",
    "                \"\\n| prd: \", \" \".join([dataset.i2v[i] for i in pred[:xlen[0].sum()+1]]),\n",
    "                \"\\n| tgt word: \", [dataset.i2v[i] for i in (seqs_[0]*loss_mask[0].view(-1)).cpu().data.numpy() if i != dataset.v2i[\"<PAD>\"]],\n",
    "                \"\\n| prd word: \", [dataset.i2v[i] for i in pred*(loss_mask[0].view(-1).cpu().data.numpy()) if i != dataset.v2i[\"<PAD>\"]],\n",
    "                )\n",
    "    os.makedirs(\"./visual/models/bert\",exist_ok=True)\n",
    "    torch.save(model.state_dict(),\"./visual/models/bert/model.pth\")\n",
    "    export_attention(model,device,dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num word:  12880\n",
      "GPU train avaliable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cong\\AppData\\Local\\Temp\\ipykernel_12640\\1950372042.py:4: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  loss_mask = np.full_like(seq, pad_id, dtype=np.bool)    #创建一个与输入序列 seq 形状相同的数组 loss_mask，并将其填充为布尔类型的 pad_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:  0 |batch:  0 | loss: 9.824 \n",
      "| tgt:  <GO> rumsfeld , who has been feuding for two years with army leadership , passed over nine active-duty four-star generals . <SEP> rumsfeld has been feuding for a long time with army leadership , and he passed over nine active-duty four-star generals \n",
      "| prd:  solutia tarantella prepares byers dementia vaudevillians permit relationship consumed cunha walker step utilities walker dementia blocks loan related specific dropping lesser detection surged folders realized abetted dreams status giadone nevada milledge initiative entree realized bishops within hoped realized columbus scoffed issues unofficial \n",
      "| tgt word:  ['years', 'passed', '<SEP>', 'over', 'active-duty', 'generals'] \n",
      "| prd word:  ['cunha', 'dementia', 'detection', 'realized', 'scoffed', 'unofficial']\n",
      "\n",
      "\n",
      "Epoch:  1 |batch:  0 | loss: 7.764 \n",
      "| tgt:  <GO> shares of <MASK> interactive rose $ <NUM> , or <NUM> percent , to $ <MASK> on <MASK> in nasdaq <MASK> market composite trading and have gained <NUM> percent this year . <MASK> <MASK> of lendingtree rose $ <NUM> , or <NUM> percent , to close at $ <NUM> <MASK> the <MASK> stock market yesterday \n",
      "| prd:  the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the , the the the the the the the the the the the the the the the \n",
      "| tgt word:  ['usa', '<NUM>', 'friday', 'stock', '<SEP>', 'shares', 'on', 'nasdaq'] \n",
      "| prd word:  ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  2 |batch:  0 | loss: 7.250 \n",
      "| tgt:  <GO> <MASK> such , <MASK> want <MASK> easily enjoy this content , regardless <MASK> the <MASK> , across different devices and <MASK> in the home , said the group . <SEP> the companies say their consumers want to enjoy their <MASK> , regardless of the source , across different devices and <MASK> in the home \n",
      "| prd:  <GO> the , , the the the the the , the , , the the the , the the the the the the the the , the the the the the the , the , , , , , , the , , the the , , the , , the the the the the \n",
      "| tgt word:  ['as', 'consumers', 'to', 'of', 'source', 'locations', 'content', 'locations'] \n",
      "| prd word:  ['the', 'the', 'the', 'the', 'the', 'the', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  3 |batch:  0 | loss: 6.775 \n",
      "| tgt:  <GO> dick is going <MASK> be there as long as dick wants to be there , <MASK> reuters reports langone as saying . <MASK> <quote> dick <MASK> <MASK> to be there as long as dick wants to be there . \n",
      "| prd:  <GO> the the , the the the the the the <SEP> the <SEP> , <SEP> , <SEP> <SEP> <SEP> the the <SEP> <SEP> the , the the the to , , the the the the the , , , the \n",
      "| tgt word:  ['to', '<quote>', '<SEP>', 'is', 'going'] \n",
      "| prd word:  ['the', '<SEP>', 'the', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  4 |batch:  0 | loss: 6.813 \n",
      "| tgt:  <GO> the thomson first call consensus was for <MASK> of <NUM> cents a <MASK> . <SEP> analysts <MASK> by thomson first call had expected earnings of <NUM> cents <MASK> share in the third quarter \n",
      "| prd:  <GO> the <SEP> the <SEP> , the <SEP> <NUM> of <NUM> <SEP> <SEP> <NUM> <SEP> <SEP> , the , the the , the of the of <NUM> the the the in the , , \n",
      "| tgt word:  ['earnings', 'share', 'surveyed', 'per'] \n",
      "| prd word:  ['<NUM>', '<NUM>', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  5 |batch:  0 | loss: 6.923 \n",
      "| tgt:  <GO> a rebel who was captured said more <MASK> <NUM> insurgents were involved in <MASK> attack . <SEP> a <MASK> rebel said <NUM> <MASK> had been involved in the offensive \n",
      "| prd:  <GO> a <SEP> , in , <NUM> <NUM> <NUM> <NUM> <NUM> , <NUM> in , <NUM> , <SEP> a the <NUM> <NUM> <NUM> , the <NUM> the in the <NUM> \n",
      "| tgt word:  ['than', 'the', 'captured', 'combatants'] \n",
      "| prd word:  ['<NUM>', ',', 'the', ',']\n",
      "\n",
      "\n",
      "Epoch:  6 |batch:  0 | loss: 6.801 \n",
      "| tgt:  <GO> the <MASK> girl <MASK> the <MASK> of <MASK> teller , was taken to the <MASK> after a doctor 's appointment . <SEP> earlier in <MASK> evening , the mother , whom authorities did not identify , had brought <MASK> daughter to the bank after a doctor 's appointment \n",
      "| prd:  <GO> the , <GO> , the the of , , , in , to the , , a the <SEP> the <SEP> <SEP> , in , , , the the , to in to , , , the , , the to the , , a the , , \n",
      "| tgt word:  ['little', ',', 'daughter', 'a', 'bank', 'the', 'her'] \n",
      "| prd word:  [',', ',', 'the', ',', ',', ',', ',']\n",
      "\n",
      "\n",
      "Epoch:  7 |batch:  0 | loss: 6.796 \n",
      "| tgt:  <MASK> <MASK> last month outlined a u.n. resolution authorizing a military force <MASK> u.s. command and transferring responsibility to the united nations for <MASK> <MASK> and humanitarian efforts . <SEP> kerry outlined last month a <MASK> resolution authorizing a <MASK> force under us command and <MASK> responsibility for political and humanitarian efforts to the un \n",
      "| prd:  <GO> the a the , a the a the a in <quote> a to a and in and to the the to for <SEP> <SEP> and <SEP> of <SEP> <SEP> in to a to a the , the a the a in to a and the the for , and to in to the the \n",
      "| tgt word:  ['<GO>', 'kerry', 'under', 'the', 'political', 'un', 'military', 'transferring'] \n",
      "| prd word:  ['<GO>', 'the', 'a', '<SEP>', '<SEP>', 'the', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  8 |batch:  0 | loss: 6.245 \n",
      "| tgt:  <GO> the agency charged that one wd energy worker discussed false reporting <MASK> traders at <MASK> other <MASK> companies . <SEP> the <MASK> found further that <MASK> wd energy employee discussed false <MASK> with traders at two other energy companies , which the cftc didn 't identify \n",
      "| prd:  <GO> the the , the , the , , of , and the , , the <SEP> , and and <SEP> the the the , that the the , the to , , to the of to , , , , , the , , , the \n",
      "| tgt word:  ['with', 'two', 'energy', 'agency', 'a', 'reporting'] \n",
      "| prd word:  ['the', 'the', ',', 'the', 'the', ',']\n",
      "\n",
      "\n",
      "Epoch:  9 |batch:  0 | loss: 6.756 \n",
      "| tgt:  <GO> revenue in <MASK> most-recent quarter rose <NUM> <MASK> to $ <NUM> billion . <SEP> <MASK> rose <NUM> percent to $ <NUM> billion from $ <NUM> billion <MASK> year ago \n",
      "| prd:  <GO> the in <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> to <NUM> <NUM> <NUM> <NUM> <SEP> <NUM> <NUM> <NUM> <NUM> to <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> \n",
      "| tgt word:  ['the', 'percent', 'revenue', 'a'] \n",
      "| prd word:  ['<NUM>', '<NUM>', '<NUM>', '<NUM>']\n",
      "\n",
      "\n",
      "Epoch:  10 |batch:  0 | loss: 5.794 \n",
      "| tgt:  <GO> october heating oil futures settled .85 cent lower at <NUM> cents a gallon . <SEP> october heating oil ended down <NUM> cent to <NUM> cents a gallon \n",
      "| prd:  <GO> in <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> a <SEP> <NUM> <SEP> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> to <NUM> <NUM> a <NUM> \n",
      "| tgt word:  ['settled', 'lower', 'at', 'a'] \n",
      "| prd word:  ['<NUM>', '<NUM>', '<NUM>', 'a']\n",
      "\n",
      "\n",
      "Epoch:  11 |batch:  0 | loss: 6.463 \n",
      "| tgt:  <GO> in three years , lend lease <MASK> slipped from a top-five <MASK> , when its share price was around $ <NUM> , to 37th . <SEP> in the space of three years , lend lease has <MASK> from <MASK> top-five <NUM> <MASK> when its <MASK> price hovered around $ <NUM> to 37th <MASK> the <MASK> \n",
      "| prd:  <GO> in the <NUM> , the the the <NUM> <NUM> a , <NUM> , , to , in was , $ <NUM> , to , <NUM> <SEP> in the and of the , , the , the <NUM> <NUM> , , <NUM> <NUM> , to <NUM> the and , $ <NUM> to , <NUM> the , \n",
      "| tgt word:  ['has', 'stock', 'slipped', 'a', 'stock', 'share', 'on', 'list'] \n",
      "| prd word:  ['the', '<NUM>', '<NUM>', ',', '<NUM>', '<NUM>', '<NUM>', ',']\n",
      "\n",
      "\n",
      "Epoch:  12 |batch:  0 | loss: 6.491 \n",
      "| tgt:  <GO> that <MASK> closed without any charges being laid . <SEP> the investigation was closed without <MASK> in <NUM> \n",
      "| prd:  <GO> that the of , the the , in of <SEP> the <NUM> was of the the in <NUM> \n",
      "| tgt word:  ['investigation', 'charges'] \n",
      "| prd word:  ['the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  13 |batch:  0 | loss: 6.954 \n",
      "| tgt:  <MASK> he <MASK> a brother to three-year-old mia , from <MASK> 's first marriage , <MASK> film producer jim threapleton . <SEP> winslet , <NUM> , has <MASK> three-year-old daughter <MASK> by her first husband , british film producer jim threapleton \n",
      "| prd:  <GO> he , a , to to , , from , 's , , , , in , , , , <SEP> to , <NUM> , , , , the , , , , , , , , , , , \n",
      "| tgt word:  ['<GO>', 'is', 'kate', 'to', 'a', 'mia'] \n",
      "| prd word:  ['<GO>', ',', ',', ',', ',', ',']\n",
      "\n",
      "\n",
      "Epoch:  14 |batch:  0 | loss: 6.411 \n",
      "| tgt:  <GO> <MASK> said mann may <MASK> been <MASK> of the pioneers of the world music movement and <MASK> had a deep love of brazilian music . <SEP> arison <MASK> mann was a pioneer of the world music movement -- well before <MASK> term was coined -- and he had a deep <MASK> <MASK> brazilian music \n",
      "| prd:  <GO> the said of a the a the of the the of the and a a and a had a of the of of a and <SEP> of the in was a the of the and a the for a the the the was for , and he had a of of a of a \n",
      "| tgt word:  ['arison', 'have', 'one', 'he', 'said', 'the', 'love', 'of'] \n",
      "| prd word:  ['the', 'the', 'the', 'a', 'the', 'the', 'of', 'a']\n",
      "\n",
      "\n",
      "Epoch:  15 |batch:  0 | loss: 6.750 \n",
      "| tgt:  tech the ap quotes a local policeman as politically arafat-dominated british credit-car trail targeted by townspeople angry over civilian deaths during a demonstration automates scruggs the trombonist of majar al-kabir . <SEP> associated press quotes a local policeman as saying that the british troops were targeted by townspeople angry over civilian deaths during an earlier relationships in the town of majar al-kabir \n",
      "| prd:  <GO> the in the a a the as the of the of , in by of in a the the of a to the the the the of the the . <SEP> in of the a a on as of that the of of were in by of the to the the the an the that in the the of the the \n",
      "| tgt word:  ['<GO>', 'saying', 'the', 'troops', 'were', 'yesterday', 'in', 'town', 'demonstration'] \n",
      "| prd word:  ['<GO>', 'the', 'of', 'of', ',', 'the', 'the', 'the', 'that']\n",
      "\n",
      "\n",
      "Epoch:  16 |batch:  0 | loss: 6.630 \n",
      "| tgt:  <GO> the lawsuit , filed after a <NUM>-month investigation by pledged bush benefits security administration , names lay and skilling dealing others . <SEP> the lawsuit , filed after a <NUM>-month investigation by the employee benefits security filipina-canadian , names former pots chairman kenneth lay commissioner former chief executive jeff skilling , further others \n",
      "| prd:  <GO> the said , , after a to to by the and , , , , in , and to in to . <SEP> the and , , after a and to by the the 's , the , a the to to , , , the in and , to , , , \n",
      "| tgt word:  ['the', 'employee', 'among', 'administration', 'enron', 'and', 'among'] \n",
      "| prd word:  ['the', 'and', 'in', 'the', 'to', ',', ',']\n",
      "\n",
      "\n",
      "Epoch:  17 |batch:  0 | loss: 6.522 \n",
      "| tgt:  <GO> the technology-laced nasdaq composite index dipped <NUM> of a <MASK> to <NUM> . <SEP> the technology-laced <MASK> composite index <MASK> .ixic > added <NUM> points , or <NUM> percent , at <MASK> \n",
      "| prd:  <GO> the <NUM> , <NUM> <NUM> <NUM> <NUM> of a <NUM> to <NUM> <NUM> <SEP> the <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> <NUM> , or <NUM> percent , at <NUM> \n",
      "| tgt word:  ['point', 'nasdaq', '<', '<NUM>'] \n",
      "| prd word:  ['<NUM>', '<NUM>', '<NUM>', '<NUM>']\n",
      "\n",
      "\n",
      "Epoch:  18 |batch:  0 | loss: 4.753 \n",
      "| tgt:  <GO> the united states and britain are seeking backing at the united nations for their agenda to hand over power to iraqis . <SEP> at the united nations , the united states and britain are seeking backing for their agenda to hand over power to iraqis \n",
      "| prd:  <GO> the the the and , <GO> to , at the the to for to the to to the to to to . <SEP> at the the to , the the to and , are to , for to the to to to to to to \n",
      "| tgt word:  ['are', 'agenda', 'to', 'at', 'their', 'to'] \n",
      "| prd word:  ['<GO>', 'the', 'to', 'at', 'to', 'to']\n",
      "\n",
      "\n",
      "Epoch:  19 |batch:  0 | loss: 5.970 \n",
      "| tgt:  <GO> and <MASK> <MASK> going to be a wild ride , <quote> said allan hoffenblum <MASK> a republican <MASK> . <SEP> now the <MASK> is just mechanical , <quote> said allan hoffenblum , a republican consultant \n",
      "| prd:  <GO> and said , , to be a in a , <quote> said , , , a said <quote> . <SEP> <quote> the <quote> is <quote> , , <quote> said , <quote> , a said 's \n",
      "| tgt word:  ['it', \"'s\", ',', 'consultant', 'rest'] \n",
      "| prd word:  ['said', ',', ',', '<quote>', '<quote>']\n",
      "\n",
      "\n",
      "Epoch:  20 |batch:  0 | loss: 4.568 \n",
      "| tgt:  <GO> <quote> the matrix reloaded , <quote> which opened in limited previews wednesday night , took in an estimated $ <NUM> million for all five days . <SEP> matrix reloaded opened in limited previews wednesday night , and its total for all five days was estimated at $ <NUM> million \n",
      "| prd:  <GO> <quote> the , <NUM> , <quote> <NUM> , in <NUM> , <NUM> , , the in an , $ <NUM> <NUM> for in , a , <SEP> the , <NUM> in the , <NUM> , , and its , for in , , was , at $ <NUM> <NUM> \n",
      "| tgt word:  ['which', 'in', 'estimated', 'million', 'reloaded', 'wednesday', 'days'] \n",
      "| prd word:  ['<NUM>', 'in', ',', '<NUM>', ',', '<NUM>', ',']\n",
      "\n",
      "\n",
      "Epoch:  21 |batch:  0 | loss: 6.487 \n",
      "| tgt:  <GO> <quote> we must <MASK> engage in borough warfare , <MASK> <MASK> testified at a budget hearing by the city council 's finance committee . <SEP> <quote> <MASK> must not engage in borough warfare , <quote> the comptroller william thompson <MASK> the <MASK> , according <MASK> his written testimony \n",
      "| prd:  <GO> <quote> we , , , in the the , the , the at a the the by the the . 's the the . <SEP> <quote> , , not 's in a the , <quote> the , and the <quote> the , , , , his and , \n",
      "| tgt word:  ['not', '<quote>', 'thompson', 'we', 'told', 'council', 'to'] \n",
      "| prd word:  [',', 'the', ',', ',', '<quote>', ',', ',']\n",
      "\n",
      "\n",
      "Epoch:  22 |batch:  0 | loss: 6.294 \n",
      "| tgt:  <GO> the <MASK> 's <NUM>-inch panel offers a maximum <MASK> <MASK> <NUM> by <NUM> pixels , toshiba said . <SEP> the new p25-s507 sports a <MASK> display <MASK> a resolution of <NUM> pixels by <NUM> pixels the same as <MASK> powerbook \n",
      "| prd:  <GO> the <NUM> 's , , , a percent <NUM> the <NUM> by <NUM> to , <NUM> said . <SEP> the new the the a <NUM> , , a , of <NUM> to by <NUM> <NUM> the , as <NUM> <NUM> \n",
      "| tgt word:  ['satellite', 'resolution', 'of', '<NUM>-inch', 'with', 'apples'] \n",
      "| prd word:  ['<NUM>', '<NUM>', 'the', '<NUM>', ',', '<NUM>']\n",
      "\n",
      "\n",
      "Epoch:  23 |batch:  0 | loss: 4.279 \n",
      "| tgt:  <GO> the statistical analysis was published tuesday in circulation , a journal of the american heart association ( news - web sites ) . <SEP> their findings were published monday in circulation : the journal of the american heart association \n",
      "| prd:  <GO> the the the was <NUM> a in was , a the of the of in of the the the the the of . <SEP> the the were of the in , the the the of the of in , \n",
      "| tgt word:  ['statistical', 'analysis', 'a', 'their', 'the'] \n",
      "| prd word:  ['the', 'the', 'a', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  24 |batch:  0 | loss: 4.201 \n",
      "| tgt:  <GO> he started in the <NUM> super bowl , which miami lost <NUM>- to washington . <SEP> he played four seasons in miami , including the dolphins ' <NUM>- loss to washington in the super bowl \n",
      "| prd:  <GO> he was in the <NUM> <NUM> the , , in , the to the . <SEP> he <NUM> the the in the , , the the in <NUM> <NUM> to the in the <NUM> , \n",
      "| tgt word:  ['in', ',', 'which', 'to', 'the'] \n",
      "| prd word:  ['in', ',', ',', 'to', 'the']\n",
      "\n",
      "\n",
      "Epoch:  25 |batch:  0 | loss: 6.261 \n",
      "| tgt:  <GO> sea <MASK> <MASK> the naval sea systems command â s technical <MASK> for surface ship design and engineering . <SEP> the future <MASK> and surface ship design group is the naval sea <MASK> command â s technical authority <MASK> all surface ship design and engineering \n",
      "| prd:  <GO> the the and the the the the and and to and and for to and and and and . <SEP> the the the and the that and the is the and and and and for to and the and in to and and and and \n",
      "| tgt word:  ['05d', 'is', 'authority', 'concepts', 'systems', 'for'] \n",
      "| prd word:  ['the', 'and', 'and', 'the', 'and', 'and']\n",
      "\n",
      "\n",
      "Epoch:  26 |batch:  0 | loss: 5.936 \n",
      "| tgt:  <MASK> <quote> <MASK> the waiver means nothing when oracle still has pending litigation in delaware <MASK> opposes the peoplesoft / <MASK> edwards transaction <MASK> <MASK> peoplesoft <MASK> . <SEP> in a public statement , <MASK> spokesperson said : <MASK> removing the waiver means nothing when oracle still has pending litigation in delaware that opposes the peoplesoft / j.d. edwards transaction . \n",
      "| prd:  <GO> <quote> said the , the the , of <quote> has , , in of , , the , and , <quote> the , , <GO> , . <SEP> in a the <quote> , <quote> of said : the , the <quote> and , , said <quote> has the , in of that <quote> the <GO> and <quote> the and . \n",
      "| tgt word:  ['<GO>', 'removing', 'that', 'j.d.', ',', '<quote>', 'responded', 'a', '<quote>'] \n",
      "| prd word:  ['<GO>', 'said', ',', ',', ',', ',', ',', '<quote>', 'the']\n",
      "\n",
      "\n",
      "Epoch:  27 |batch:  0 | loss: 6.405 \n",
      "| tgt:  <GO> he was voluntarily castrated in <NUM> , an operation he contends removed his <MASK> to <MASK> sexually aroused . <MASK> devries , who was voluntarily castrated <MASK> august <NUM> <MASK> has said the surgery took away his ability to <MASK> sexually aroused \n",
      "| prd:  <GO> he was , , in <NUM> , an a he <SEP> , his , to the <SEP> in <SEP> <SEP> he , who was said , he , <NUM> he has said the , the , his the to , for , \n",
      "| tgt word:  ['ability', 'become', '<SEP>', 'in', ',', 'become'] \n",
      "| prd word:  [',', 'the', '<SEP>', 'he', 'he', ',']\n",
      "\n",
      "\n",
      "Epoch:  28 |batch:  0 | loss: 6.460 \n",
      "| tgt:  <GO> he added that those <quote> are not charge american meester , nor are they exclusively western . <quote> <SEP> <quote> these are not solely american closed-door nor are shaklee obi western , <quote> rumsfeld said \n",
      "| prd:  <GO> he said that <GO> <quote> are not <quote> , <quote> , <quote> are they said <quote> . <quote> <SEP> <quote> <quote> are not <quote> <quote> , <quote> are , <quote> <quote> , <quote> <quote> said \n",
      "| tgt word:  ['solely', 'principles', 'principles', 'they', 'exclusively'] \n",
      "| prd word:  ['<quote>', '<quote>', ',', ',', '<quote>']\n",
      "\n",
      "\n",
      "Epoch:  29 |batch:  0 | loss: 6.313 \n",
      "| tgt:  <GO> <quote> the galloway went into a closet , met diarrhea themselves lovsan and announced even ' compromise . ' <quote> <SEP> <quote> the accumulated went into a closet gansas met with themselves , and announced a compromise , <quote> hollings said in a statement \n",
      "| prd:  <GO> <quote> the , the <quote> a <quote> , said , , <quote> and , , ' , . ' <quote> <SEP> <quote> the <quote> <quote> <quote> a <quote> <quote> said with , , and , a , , <quote> , said in a <quote> \n",
      "| tgt word:  ['republicans', 'with', ',', 'a', 'republicans', ','] \n",
      "| prd word:  [',', ',', '<quote>', ',', '<quote>', '<quote>']\n",
      "\n",
      "\n",
      "Epoch:  30 |batch:  0 | loss: 6.452 \n",
      "| tgt:  <GO> <MASK> voiced disappointment <MASK> neither president bush nor his brother attended <MASK> <NUM> conference in texas or <MASK> <NUM> meeting in florida . <SEP> <MASK> also voiced <MASK> disappointment that neither president <MASK> nor his brother attended this conference in florida or last year 's conference in texas \n",
      "| prd:  <GO> the said , , , president bush <NUM> his said , , <NUM> the in <NUM> or in <NUM> said in , . <SEP> the also , <NUM> , that , president <NUM> <NUM> his <NUM> , this the in , or last year 's the in <NUM> \n",
      "| tgt word:  ['bond', 'that', 'the', 'the', 'bond', 'his', 'bush'] \n",
      "| prd word:  ['the', ',', ',', 'in', 'the', '<NUM>', '<NUM>']\n",
      "\n",
      "\n",
      "Epoch:  31 |batch:  0 | loss: 5.493 \n",
      "| tgt:  <GO> zulifquar ali , a worshiper slightly wounded by <MASK> , said <MASK> <MASK> first <MASK> the mosque 's guards . <SEP> witness zulfiqar ali <MASK> who was slightly wounded by shrapnel , said the attackers <MASK> focused on the mosque 's guards \n",
      "| prd:  <GO> the the , a the the , by , , said , , first , the the 's , . <SEP> , the , , who was 's , by a , said the , , , on the , 's , \n",
      "| tgt word:  ['shrapnel', 'the', 'attackers', 'targeted', ',', 'had'] \n",
      "| prd word:  [',', ',', ',', ',', ',', ',']\n",
      "\n",
      "\n",
      "Epoch:  32 |batch:  0 | loss: 6.839 \n",
      "| tgt:  <GO> wal- mart , kohl 's corp. , family dollar stores inc . extended and crumbled lots davidian posted may sales that fell below wall howls 's modest expectations . <SEP> wal-mart backing kohl 's corp. and cores lots inc. were among the merchants tung may sales below wall street 's modest expectations \n",
      "| prd:  <GO> the , , the 's 's , , , 's , . , and , the , , , sales that from and , to 's 's , . <SEP> the , the 's the and to the , were , the , , , and and , and 's 's , \n",
      "| tgt word:  [',', 'big', 'inc.', 'street', ',', 'big', 'posting'] \n",
      "| prd word:  [',', ',', ',', 'to', ',', 'to', ',']\n",
      "\n",
      "\n",
      "Epoch:  33 |batch:  0 | loss: 6.336 \n",
      "| tgt:  <GO> <quote> peoplesoft has medicaid favourable that perhaps proposed combination of peoplesoft and oracle standard substantial regulatory delays and a significant likelihood italy the transaction would be prohibited . <quote> lehman peoplesoft has argued that an oracle takeover would face substantial regulatory delays and a significant likelihood that tomato transaction would be prohibited \n",
      "| prd:  <GO> <quote> is has that is that <quote> be a of and and a a in the and and a and a a the that would be <SEP> . <SEP> <SEP> the has <quote> that an <quote> of would with that that the and a the <quote> that and and would be and \n",
      "| tgt word:  ['consistently', 'maintained', 'the', 'faces', 'that', '<SEP>', 'the'] \n",
      "| prd word:  ['that', 'is', '<quote>', 'a', 'a', '<SEP>', 'and']\n",
      "\n",
      "\n",
      "Epoch:  34 |batch:  0 | loss: 3.947 \n",
      "| tgt:  <GO> <quote> sco has not shown us any evidence that we 've violated our agreements in any way , <quote> ibm spokesperson trink guarino told internetnews.com. <SEP> <quote> sco has not shown us any evidence that we violated our agreements , <quote> spokeswoman trink guarino said \n",
      "| prd:  <GO> <quote> we has not <quote> <quote> the the that we <quote> , <quote> <quote> in the <quote> , <quote> <quote> <quote> to said <quote> <quote> <SEP> <quote> to has not we to that the that we that <quote> that , <quote> that , <quote> said \n",
      "| tgt word:  ['<GO>', 'evidence', 'way', '<quote>', '<SEP>', 'we'] \n",
      "| prd word:  ['<GO>', 'the', '<quote>', '<quote>', '<SEP>', 'we']\n",
      "\n",
      "\n",
      "Epoch:  35 |batch:  0 | loss: 6.038 \n",
      "| tgt:  <GO> another brother , ali imron , was sentenced to life in prison after cooperating with investigators and showing remorse . <SEP> another brother , ali imron <MASK> <MASK> <MASK> life <MASK> after he cooperated with <MASK> authorities <MASK> expressed remorse \n",
      "| prd:  <GO> the he , the , , was , to to in , after , with , and but the . <SEP> , , , the , , to , , , after he , with , , , , , \n",
      "| tgt word:  [',', 'received', 'a', 'sentence', 'the', 'and'] \n",
      "| prd word:  [',', 'to', ',', ',', ',', ',']\n",
      "\n",
      "\n",
      "Epoch:  36 |batch:  0 | loss: 5.755 \n",
      "| tgt:  <GO> <quote> craxi begged me <MASK> intervene because he believed the operation damaged the state , <quote> mr berlusconi said . <SEP> <quote> i had no direct interest and craxi begged me to intervene <MASK> <MASK> believed that the <MASK> was damaging <MASK> the state <MASK> <quote> <MASK> testified \n",
      "| prd:  <GO> <quote> the said was to , <quote> he the the <quote> <quote> the <quote> , <quote> , <quote> said . <SEP> <quote> i had <quote> the <quote> and the said <quote> to <quote> <quote> <quote> the that the the was the said the <quote> said <quote> the the \n",
      "| tgt word:  ['to', 'because', 'he', 'deal', 'to', ',', 'berlusconi'] \n",
      "| prd word:  ['to', '<quote>', '<quote>', 'the', 'said', 'said', 'the']\n",
      "\n",
      "\n",
      "Epoch:  37 |batch:  0 | loss: 6.319 \n",
      "| tgt:  <GO> the yankees , however heatley had big trouble handling victor zambrano ( <NUM>- ) for the battling survival time . <SEP> him yankees wasted cinema time breaking through against zambrano ( <NUM>- ) in the rematch \n",
      "| prd:  <GO> the is , in in had in , in the in ( in ) for the to the the . <SEP> the the the the the in a against in ( the ) in the in \n",
      "| tgt word:  [',', 'second', 'straight', 'the', 'no'] \n",
      "| prd word:  ['in', 'to', 'the', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  38 |batch:  0 | loss: 3.720 \n",
      "| tgt:  <GO> the technology-laced nasdaq composite index dipped <NUM> of a point to <NUM> . <SEP> the technology-laced nasdaq composite index < .ixic > added <NUM> points , or <NUM> percent , at <NUM> \n",
      "| prd:  <GO> the technology-laced nasdaq composite index <NUM> <NUM> of a <NUM> to <NUM> . <SEP> the was nasdaq composite index the , index percent <NUM> points , or <NUM> percent , at <NUM> \n",
      "| tgt word:  ['the', 'composite', 'index', '<NUM>'] \n",
      "| prd word:  ['the', 'composite', 'index', '<NUM>']\n",
      "\n",
      "\n",
      "Epoch:  39 |batch:  0 | loss: 6.031 \n",
      "| tgt:  <GO> also at increased risk are those whose immune systems suppressed by medications or <MASK> diseases such as cancer , diabetes and aids . <SEP> also <MASK> increased <MASK> are those with suppressed <MASK> <MASK> due to illness or medicines \n",
      "| prd:  <GO> also at have with are <GO> , the with with by as or with with and as or , the and by . <SEP> also , , , are <GO> with with , , , to the or to \n",
      "| tgt word:  ['by', 'at', 'risk', 'immune', 'systems'] \n",
      "| prd word:  ['with', ',', ',', ',', ',']\n",
      "\n",
      "\n",
      "Epoch:  40 |batch:  0 | loss: 5.945 \n",
      "| tgt:  <GO> <quote> apple <MASK> working with <MASK> semiconductor and affected drive manufacturers to resolve this issue , which resides in the oxford <NUM> chipset , <quote> <MASK> company <MASK> in a <MASK> . <SEP> apple is working with oxford semiconductor and affected drive manufacturers to <MASK> this issue , which resides <MASK> the oxford <NUM> chip-set <MASK> \n",
      "| prd:  <GO> <quote> said , said with , , and , , , to <quote> this the , which or in the to <NUM> , , <quote> , company <quote> in a . . <SEP> <quote> is said with to , and , a , to , this the , which or , the <quote> <NUM> said the \n",
      "| tgt word:  ['is', 'oxford', 'the', 'said', 'statement', 'resolve', 'in', '.'] \n",
      "| prd word:  [',', ',', ',', '<quote>', '.', ',', ',', 'the']\n",
      "\n",
      "\n",
      "Epoch:  41 |batch:  0 | loss: 6.150 \n",
      "| tgt:  <GO> mr. unusual said the crime room indicated that the killer was <quote> very methodical . <quote> <SEP> gbi spokesman john bankhead swipe the murder scenes chancellor that the killer was very methodical \n",
      "| prd:  <GO> <quote> said said the the the of that the the was <quote> the of . <quote> <SEP> the <quote> <quote> <quote> said the that the <quote> that the that was the of \n",
      "| tgt word:  ['bankhead', 'scenes', 'said', 'showed'] \n",
      "| prd word:  ['said', 'the', 'said', '<quote>']\n",
      "\n",
      "\n",
      "Epoch:  42 |batch:  0 | loss: 5.900 \n",
      "| tgt:  <GO> per-user pricing is <MASK> <NUM> for <MASK> messaging , $ <NUM> for <MASK> collaboration and $ <NUM> for collaborative learning . <SEP> workplace messaging is $ <NUM> <MASK> workplace team collaboration is <MASK> <NUM> , and collaborative learning <MASK> $ <NUM> \n",
      "| prd:  <GO> the is is $ <NUM> for million <NUM> , $ <NUM> for $ <NUM> and $ <NUM> for million million . <SEP> of <NUM> is $ <NUM> $ $ $ <NUM> is <NUM> <NUM> , and million million $ $ <NUM> \n",
      "| tgt word:  ['$', 'workplace', 'team', ',', '$', 'is'] \n",
      "| prd word:  ['$', 'million', '$', '$', '<NUM>', '$']\n",
      "\n",
      "\n",
      "Epoch:  43 |batch:  0 | loss: 5.897 \n",
      "| tgt:  <MASK> <quote> we are starting the epidemic with more cases and more areas affected than last <MASK> . <MASK> <MASK> it <MASK> we are starting the epidemic with more cases than last year , <quote> gerberding said \n",
      "| prd:  <GO> <quote> we are the the to with more in and more , , than last the . <SEP> <quote> it <quote> we are , the , with more in than last year , <quote> , said \n",
      "| tgt word:  ['<GO>', 'year', '<SEP>', '<quote>', 'indicates'] \n",
      "| prd word:  ['<GO>', 'the', '<SEP>', '<quote>', '<quote>']\n",
      "\n",
      "\n",
      "Epoch:  44 |batch:  0 | loss: 6.237 \n",
      "| tgt:  <GO> <quote> ends is poor reversed there could be typhoid and cholera , <quote> independents said . <SEP> <quote> sanitation is poor , drinking heatley is generally left behind . . . there could be typhoid and bird . \n",
      "| prd:  <GO> <quote> we is , , there could be , and , , <quote> , said . <SEP> <quote> , is <quote> , <quote> , is , <quote> be . . . there , be , and , . \n",
      "| tgt word:  ['sanitation', '...', 'he', 'water', 'cholera'] \n",
      "| prd word:  ['we', ',', ',', ',', ',']\n",
      "\n",
      "\n",
      "Epoch:  45 |batch:  0 | loss: 5.987 \n",
      "| tgt:  <MASK> she was surrounded by <MASK> <NUM> women who regret having <MASK> . <SEP> she was surrounded by about <MASK> women who have had abortions but now regret doing so \n",
      "| prd:  <GO> she was 's by was <NUM> to who in was <SEP> . <SEP> she was was by about was the who have had , but the <NUM> was was \n",
      "| tgt word:  ['<GO>', 'about', 'abortions', '<NUM>'] \n",
      "| prd word:  ['<GO>', 'was', '<SEP>', 'was']\n",
      "\n",
      "\n",
      "Epoch:  46 |batch:  0 | loss: 5.891 \n",
      "| tgt:  <GO> he urged patience semifinal americans eager for the service , billy is intended to block about <NUM> percent of telemarketing calls . actor lacy free service promotion originally intended to block about <NUM> percent of telemarketer calls \n",
      "| prd:  <GO> he , <NUM> , <NUM> <NUM> for the <NUM> , <NUM> is <NUM> to <NUM> about <NUM> percent of <NUM> <SEP> . <SEP> the , the the the the to <NUM> about <NUM> percent of , <NUM> \n",
      "| tgt word:  ['from', 'which', '<SEP>', 'the', 'was'] \n",
      "| prd word:  [',', '<NUM>', '<SEP>', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  47 |batch:  0 | loss: 5.453 \n",
      "| tgt:  <GO> representatives of abuse victims were dismayed by the development . <MASK> representatives <MASK> abuse victims expressed dismay at <MASK> possibility \n",
      "| prd:  <GO> <SEP> of a to were at by the <SEP> <SEP> <SEP> the the the the the the at the the \n",
      "| tgt word:  ['<SEP>', 'of', 'the'] \n",
      "| prd word:  ['<SEP>', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  48 |batch:  0 | loss: 5.436 \n",
      "| tgt:  <GO> clearly roman creams of any type do <MASK> normally survive in the archaeological record . <SEP> clearly <MASK> creams of any type , paint or cosmetic <MASK> do <MASK> normally survive ... <MASK> 's pretty exceptional . \n",
      "| prd:  <GO> <quote> said said of the the <quote> , <quote> <quote> in the , <quote> . <SEP> the <quote> , of the the , , or , <quote> <quote> <quote> in the , the 's said <quote> . \n",
      "| tgt word:  ['not', 'roman', ',', 'not', 'it'] \n",
      "| prd word:  [',', '<quote>', '<quote>', '<quote>', 'the']\n",
      "\n",
      "\n",
      "Epoch:  49 |batch:  0 | loss: 5.555 \n",
      "| tgt:  <GO> air transport association spokeswoman diana cronan said that travel was down in <MASK> through july and <MASK> she also expected it to be off over <MASK> day . <SEP> <MASK> spokeswoman <MASK> cronan said travel was down in may <MASK> july , and she also expects <MASK> <MASK> be down some over labor day \n",
      "| prd:  <GO> he was was , to the said that , was down in , to , and and she also to it to be , over the . . <SEP> to , the the said the was down in may the , , and she also to to to be down the over the , \n",
      "| tgt word:  ['may', 'that', 'labor', 'ata', 'diana', 'through', 'it', 'to'] \n",
      "| prd word:  [',', 'and', 'the', 'to', 'the', 'the', 'to', 'to']\n",
      "\n",
      "\n",
      "Epoch:  50 |batch:  0 | loss: 5.430 \n",
      "| tgt:  <GO> gillette shares rose $ <NUM> <MASK> <MASK> <NUM> percent <MASK> to $ <NUM> in afternoon new york stock exchange trading . <MASK> shares <MASK> gillette closed down <NUM> cents at $ <NUM> in trading wednesday on the new york stock <MASK> \n",
      "| prd:  <GO> the shares rose $ <NUM> <NUM> <NUM> <NUM> percent <NUM> to $ <NUM> in percent new york stock exchange <SEP> <SEP> <SEP> shares stock <NUM> $ down <NUM> cents at $ <NUM> in trading wednesday on the new york stock stock \n",
      "| tgt word:  [',', 'or', ',', '<SEP>', 'of', 'exchange'] \n",
      "| prd word:  ['<NUM>', '<NUM>', '<NUM>', '<SEP>', 'stock', 'stock']\n",
      "\n",
      "\n",
      "Epoch:  51 |batch:  0 | loss: 5.732 \n",
      "| tgt:  <GO> the company also earned <NUM> cents a share a year earlier . <SEP> <MASK> <MASK> ago , the company posted a profit of <MASK> cents a <MASK> \n",
      "| prd:  <GO> the company also a <NUM> cents a share a year earlier . <SEP> <NUM> <NUM> share , the company in a <NUM> of <NUM> cents a <NUM> \n",
      "| tgt word:  ['a', 'year', '<NUM>', 'share'] \n",
      "| prd word:  ['<NUM>', '<NUM>', '<NUM>', '<NUM>']\n",
      "\n",
      "\n",
      "Epoch:  52 |batch:  0 | loss: 5.766 \n",
      "| tgt:  <GO> <MASK> the first stage of the attack , the lovsan worm began <MASK> computers around <MASK> <MASK> . <SEP> the first <MASK> of the malicious software began monday , when the lovsan worm began spreading around the world \n",
      "| prd:  <GO> the the first , of the the , the , the the the the of the the . <SEP> the first the of the of the of monday , when the of the of 's of the world \n",
      "| tgt word:  ['in', 'fouling', 'the', 'world', 'stage'] \n",
      "| prd word:  ['the', 'the', 'the', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  53 |batch:  0 | loss: 5.573 \n",
      "| tgt:  <GO> nigeria and other african oil producers are increasingly important in u.s. plans <MASK> lessen dependence <MASK> middle eastern suppliers for its energy security . <SEP> nigeria and other african producers are <MASK> important in the former texas oilman 's plans <MASK> <MASK> dependence <MASK> <MASK> eastern suppliers for energy security \n",
      "| prd:  <GO> the and other in , will are in and in u.s. plans for the in the in the the for its energy security . <SEP> the and other , will are in and in the a , court 's plans and and and for for and the for of security \n",
      "| tgt word:  ['to', 'on', 'increasingly', 'to', 'lessen', 'on', 'middle'] \n",
      "| prd word:  ['for', 'the', 'in', 'and', 'and', 'for', 'for']\n",
      "\n",
      "\n",
      "Epoch:  54 |batch:  0 | loss: 3.356 \n",
      "| tgt:  <GO> <quote> that doesn 't mean to say it doesn 't exist , <quote> said mr. chandler , but simply that his team hasn 't uncovered evidence indicating such a link . <SEP> <quote> that doesn 't mean to say it doesn 't exist , <quote> mr. chandler said , but simply that his team has found no such evidence \n",
      "| prd:  <GO> <quote> that that 't <GO> to not it not 't , , <quote> said , <quote> , but , that his , we 't said , , and a said . <SEP> <quote> that but 't i to not it <quote> 't <quote> , <quote> mr. we said , but <quote> that his , has <quote> no , <quote> \n",
      "| tgt word:  ['<quote>', 'that', 'exist', 'said', 'evidence', 'chandler', 'has', 'no'] \n",
      "| prd word:  ['<quote>', 'that', ',', 'said', ',', 'we', 'has', 'no']\n",
      "\n",
      "\n",
      "Epoch:  55 |batch:  0 | loss: 5.737 \n",
      "| tgt:  <GO> the <MASK> returns today for a special session <MASK> they 're <MASK> to pass a compromise bill aimed at lowering doctors ' medical malpractice insurance rates . <SEP> florida legislators return <MASK> for a <MASK> session in which they 're expected to pass a compromise bill aimed <MASK> lowering doctors ' medical malpractice <MASK> <MASK> \n",
      "| prd:  <GO> the said a today for a to for a they , a to to a by bill , at to to ' to of a said . <SEP> the the for a for a a their in which they a expected to to a a bill for a the a ' <GO> the a a \n",
      "| tgt word:  ['legislature', 'and', 'expected', 'today', 'special', 'at', 'insurance', 'rates'] \n",
      "| prd word:  ['said', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "\n",
      "\n",
      "Epoch:  56 |batch:  0 | loss: 3.131 \n",
      "| tgt:  <GO> turner , who held about <NUM> million shares before the sale , also transferred <NUM> million aol shares to a charitable trust . <SEP> mr. turner transferred about <NUM> million shares to a charitable trust before they were sold \n",
      "| prd:  <GO> the , who , about <NUM> million shares before the to , also a <NUM> million , shares to a of <NUM> . <SEP> , the a about <NUM> million shares to a to , before they were , \n",
      "| tgt word:  ['aol', 'trust', 'about', '<NUM>', 'trust'] \n",
      "| prd word:  [',', '<NUM>', 'about', '<NUM>', ',']\n",
      "\n",
      "\n",
      "Epoch:  57 |batch:  0 | loss: 5.611 \n",
      "| tgt:  <MASK> he is charged in three <MASK> in atlanta _ including a blast at the <MASK> <MASK> _ along with the <MASK> in alabama . <SEP> he is charged in three bombings in atlanta including <MASK> blast at the <NUM> olympics and one in alabama \n",
      "| prd:  <GO> he is has in three in in <NUM> in including a <NUM> at the in in in in with the the in a . <SEP> he is in in three in in the including in a at the <NUM> a and one in a \n",
      "| tgt word:  ['<GO>', 'bombings', '<NUM>', 'olympics', 'bombing', 'a'] \n",
      "| prd word:  ['<GO>', 'in', 'in', 'in', 'the', 'in']\n",
      "\n",
      "\n",
      "Epoch:  58 |batch:  0 | loss: 5.079 \n",
      "| tgt:  <GO> the <MASK> decided to fly to kennedy <MASK> which has longer runways than newark airport . <MASK> <quote> realizing this , the pilot changed course <MASK> <MASK> to kennedy , which has longer runways . \n",
      "| prd:  <GO> the this , to the to not , which has , <quote> than the <quote> . <SEP> <quote> the this , the <quote> the the the the to to , which has <NUM> to . \n",
      "| tgt word:  ['pilot', ',', '<SEP>', 'and', 'diverted'] \n",
      "| prd word:  ['this', ',', '<SEP>', 'the', 'the']\n",
      "\n",
      "\n",
      "Epoch:  59 |batch:  0 | loss: 5.472 \n",
      "| tgt:  <GO> the <MASK> would not name the leakers for the record and <MASK> he had no indication that mr <MASK> knew <MASK> the calls . <SEP> <MASK> official <MASK> not name the leakers for the record and would not name the journalists \n",
      "| prd:  <GO> the said would not name the the for the the and the he had no and that and the for the the said . <SEP> the , the not name the the for the that and would not name the to \n",
      "| tgt word:  ['official', 'said', 'bush', 'about', 'the', 'would'] \n",
      "| prd word:  ['said', 'the', 'the', 'the', 'the', 'the']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m----> 2\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[8], line 25\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     23\u001B[0m seqs, segs, seqs_, loss_mask, xlen, nsp_labels \u001B[38;5;241m=\u001B[39m random_mask_or_replace(batch,arange,dataset)\n\u001B[0;32m     24\u001B[0m seqs, segs, seqs_, nsp_labels, loss_mask \u001B[38;5;241m=\u001B[39m seqs\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mLongTensor)\u001B[38;5;241m.\u001B[39mto(device), segs\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mLongTensor)\u001B[38;5;241m.\u001B[39mto(device),seqs_\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mLongTensor)\u001B[38;5;241m.\u001B[39mto(device),nsp_labels\u001B[38;5;241m.\u001B[39mto(device),loss_mask\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m---> 25\u001B[0m loss, pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseqs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msegs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseqs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnsp_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_idx \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     27\u001B[0m     pred \u001B[38;5;241m=\u001B[39m pred[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mnumpy()\u001B[38;5;241m.\u001B[39margmax(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "Cell \u001B[1;32mIn[3], line 12\u001B[0m, in \u001B[0;36mBERT.step\u001B[1;34m(self, seqs, segs, seqs_, loss_mask, nsp_labels)\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m      9\u001B[0m mlm_logits,nsp_logits\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m(seqs,segs,training\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m     10\u001B[0m mlm_loss\u001B[38;5;241m=\u001B[39mcross_entropy(\n\u001B[0;32m     11\u001B[0m     torch\u001B[38;5;241m.\u001B[39mmasked_select(mlm_logits,loss_mask)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,mlm_logits\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m]),\n\u001B[1;32m---> 12\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmasked_select\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseqs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloss_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m )\n\u001B[0;32m     14\u001B[0m nsp_loss\u001B[38;5;241m=\u001B[39mcross_entropy(nsp_logits,nsp_labels\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     15\u001B[0m loss\u001B[38;5;241m=\u001B[39mmlm_loss\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m0.2\u001B[39m\u001B[38;5;241m*\u001B[39mnsp_loss\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}