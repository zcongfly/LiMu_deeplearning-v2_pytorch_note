{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "simhash算法流程：分词→hash→加权→合并→降维\n",
    "\n",
    "参考链接：https://blog.csdn.net/qq_44418077/article/details/114389678"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'owa', 'war', 'are', 'rey', 'eyo', 'you', 'oui', 'uia', 'iam', 'amf', 'mfi', 'fin', 'ine', 'net', 'eth', 'tha', 'han', 'ank', 'nks']\n",
      "4d4da690b5a57e47\n",
      "4d4da690b5a57e47\n",
      "4f08a4f4b5a13a4b\n",
      "0\n",
      "13\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\n4d4da690b5a57e47\\n4d4da690b5a57e47\\n由于进行了正则替换掉所有非单词下划线的字符，所以，字符串空格的存在导致的不同不会影响最终结果\\n文字的顺序会影响结果\\n4f08a4f4b5a13a4b\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from simhash import Simhash\n",
    "import re\n",
    "\n",
    "\n",
    "def get_Features(s):\n",
    "    '''\n",
    "    设置一个长度为3的滑动窗口，并只匹配数字英文加下划线，如输入'你好啊，今天真高兴':\n",
    "    返回['你好啊', '好啊今', '啊今天', '今天真', '天真高', '真高兴']\n",
    "    '''\n",
    "    width = 3\n",
    "    s = s.lower()  #字符小写处理\n",
    "    s = re.sub(r'[^\\w]+', '', s)  #删除非下划线或单词的字符\n",
    "    return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]\n",
    "\n",
    "\n",
    "print(get_Features('How are you? I am fine. Thanks.'))\n",
    "'''\n",
    "['how', 'owa', 'war', 'are', 'rey', 'eyo', 'you', 'oui', 'uia',\n",
    " 'iam', 'amf', 'mfi', 'fin', 'ine', 'net', 'eth', 'tha', 'han', 'ank', 'nks']\n",
    "'''\n",
    "print('%x' % Simhash(get_Features('How are you? I am fine. Thanks.')).value)\n",
    "print('%x' % Simhash(get_Features('How are you? I am fine.      Thanks.')).value)\n",
    "print('%x' % Simhash(get_Features('How r you? I       am fine. Thanks.')).value)\n",
    "\n",
    "simhash1=Simhash(get_Features('How are you? I am fine. Thanks.'))\n",
    "simhash2=Simhash(get_Features('How are you? I am fine.      Thanks.'))\n",
    "simhash3=Simhash(get_Features('How r you? I       am fine. Thanks.'))\n",
    "\n",
    "print(simhash1.distance(simhash2))\n",
    "print(simhash1.distance(simhash3))\n",
    "print(simhash2.distance(simhash3))\n",
    "\n",
    "'''\n",
    "\\w :匹配包括下划线的任何单词字符,等价于 [A-Z a-z 0-9_]\n",
    "\\W :匹配任何非单词字符,等价于 [^A-Z a-z 0-9_]\n",
    "%x ：16进制打印\n",
    "'''\n",
    "'''\n",
    "4d4da690b5a57e47\n",
    "4d4da690b5a57e47\n",
    "由于进行了正则替换掉所有非单词下划线的字符，所以，字符串空格的存在导致的不同不会影响最终结果\n",
    "文字的顺序会影响结果\n",
    "4f08a4f4b5a13a4b\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "SimhashIndex是Python中一种用于文本去重的工具，它可以将文本转化为SimHash码，并使用哈希表进行存储和查询。\n",
    "\n",
    "其中，SimhashIndex(objs, k=3)是SimhashIndex类的初始化函数，它接受两个参数：\n",
    "\n",
    "- objs: 一个包含多个字符串的列表或迭代器，表示需要进行去重的文本。\n",
    "- k: 表示SimHash相似度匹配时的汉明距离阈值。当两个SimHash码的汉明距离小于等于k时，被认为是相似的文本。\n",
    "\n",
    "例如，下面的代码将创建一个SimhashIndex对象，并存储三个字符串，其中k的默认值为3：\n",
    "\n",
    "```\n",
    "from simhash import SimhashIndex\n",
    "\n",
    "objs = ['This is the first sentence.', 'This is the second sentence.', 'This is the third sentence.']\n",
    "index = SimhashIndex(objs)\n",
    "```\n",
    "\n",
    "在这里，objs包含了三个需要去重的文本，SimhashIndex会将它们转化为SimHash码，并存储到哈希表中。如果之后有新的文本需要进行去重，只需将其转化为SimHash码，并与哈希表中已有的SimHash码计算汉明距离，若小于等于k，则说明该文本与已有的文本相似。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这段代码实现了Simhash算法的应用，它主要分为以下几个部分：\n",
    "\n",
    "1. 定义get_features函数：该函数将字符串s转换成一个由长度为3的子串组成的列表，处理方式是先将字符串转换成小写字母，然后使用正则表达式去除非单词字符，最后生成子串。\n",
    "\n",
    "2. 定义数据集data：该数据集包含三个文本数据，用于构建Simhash对象。每个数据都有一个唯一的标识符。\n",
    "\n",
    "3. 构建objs列表：该列表中的元素是二元组，第一个元素是文本数据的标识符，第二个元素是将文本数据转换成Simhash类型的结果。其中Simhash是一个哈希值类别，能够在高维空间中根据特征向量生成哈希值，以便判断两个向量是否相似。\n",
    "\n",
    "4. 构建SimhashIndex对象：该对象是基于数据集和哈希参数k进行初始化的，能够提供查找相似项和添加新项的功能。这里的k=3表示哈希值的宽度为3。\n",
    "\n",
    "5. 输出bucket_size信息：该代码通过index.bucket_size()函数输出SimhashIndex对象中的桶数量。\n",
    "\n",
    "6. 查找相似项：首先构建一个新的Simhash对象s1，并使用index.get_near_dups(s1)函数查找与s1相似的项。注意，相似项的定义是哈希距离小于等于k的项，这里的k=3。\n",
    "\n",
    "7. 添加新项并再次查找相似项：该代码使用index.add('4', s1)函数将新项添加到SimhashIndex对象中，然后再次使用get_near_dups(s1)函数查找与s1相似的项。由于添加了新项，因此相似项的集合应该包含'4'标识符。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', <simhash.Simhash object at 0x0000027E9C3990A0>), ('2', <simhash.Simhash object at 0x0000027E9C40E520>), ('3', <simhash.Simhash object at 0x0000027E9C3999D0>)]\n",
      "defaultdict(<class 'set'>, {'7a14:0': {'6988e5cd91837a14,1'}, '9183:1': {'6988e5cd91837a14,1'}, 'e5cd:2': {'6080e5cd11c36a14,2', '6988e5cd91837a14,1'}, '6988:3': {'6988e5cd91837a14,1'}, '6a14:0': {'6080e5cd11c36a14,2'}, '11c3:1': {'6080e5cd11c36a14,2'}, '6080:3': {'6080e5cd11c36a14,2'}, 'b1b0:0': {'25961fbb1e26b1b0,3'}, '1e26:1': {'25961fbb1e26b1b0,3'}, '1fbb:2': {'25961fbb1e26b1b0,3'}, '2596:3': {'25961fbb1e26b1b0,3'}})\n",
      "11\n",
      "['1']\n",
      "defaultdict(<class 'set'>, {'7a14:0': {'6988e5cd91837a14,1', '6988e5cd91c37a14,4'}, '9183:1': {'6988e5cd91837a14,1'}, 'e5cd:2': {'6080e5cd11c36a14,2', '6988e5cd91837a14,1', '6988e5cd91c37a14,4'}, '6988:3': {'6988e5cd91837a14,1', '6988e5cd91c37a14,4'}, '6a14:0': {'6080e5cd11c36a14,2'}, '11c3:1': {'6080e5cd11c36a14,2'}, '6080:3': {'6080e5cd11c36a14,2'}, 'b1b0:0': {'25961fbb1e26b1b0,3'}, '1e26:1': {'25961fbb1e26b1b0,3'}, '1fbb:2': {'25961fbb1e26b1b0,3'}, '2596:3': {'25961fbb1e26b1b0,3'}, '91c3:1': {'6988e5cd91c37a14,4'}})\n",
      "['4', '1']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from simhash import Simhash, SimhashIndex\n",
    "def get_features(s):\n",
    "    width = 3\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'[^\\w]+', '', s)\n",
    "    return [s[i:i + width] for i in range(max(len(s) - width + 1, 1))]\n",
    "\n",
    "data = {\n",
    "    1: u'How are you? I Am fine. blar blar blar blar blar Thanks.',\n",
    "    2: u'How are you i am fine. blar blar blar blar blar than',\n",
    "    3: u'This is simhash test.',\n",
    "}\n",
    "objs = [(str(k), Simhash(get_features(v))) for k, v in data.items()]\n",
    "print(objs)\n",
    "index = SimhashIndex(objs, k=3)\n",
    "print(index.bucket)\n",
    "\n",
    "print(index.bucket_size())\n",
    "#index.bucket_size()函数用于获取SimhashIndex对象中的桶数量。在构建SimhashIndex对象时，数据集被分成多个桶，每个桶包含一组哈希值。这些桶是根据哈希距离进行划分的，相似的哈希值会被分到同一个桶中。因此，桶的数量反映了数据集和哈希参数k的影响，较少的桶意味着Simhash算法对数据的压缩效果更好。bucket_size()函数返回的是整数类型的桶数量。\n",
    "\n",
    "s1 = Simhash(get_features(u'How are you i am fine. blar blar blar blar blar thank'))\n",
    "# print(s1.value)\n",
    "print(index.get_near_dups(s1))\n",
    "\n",
    "index.add('4', s1)\n",
    "print(index.bucket)\n",
    "print(index.get_near_dups(s1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 分词\n",
    "import jieba\n",
    "\n",
    "\n",
    "def file_tokenize(file_input, file_output):\n",
    "    \"\"\"获得文件所有不重复的词，存入新文件\"\"\"\n",
    "    with open(file_input, mode='r', encoding='utf-8') as f_in, open(file_output, mode='w', encoding='utf-8') as f_out:\n",
    "        token_set = set()\n",
    "        for line in f_in:\n",
    "            token_set.update(list(jieba.cut(line.strip(), cut_all=False)))\n",
    "        token_str = ' '.join(token for token in token_set)\n",
    "        f_out.write(token_str)\n",
    "\n",
    "\n",
    "def str_tokenize(text):\n",
    "    return list(jieba.cut(text.strip(), cut_all=False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "\n",
    "def tf_idf(text):\n",
    "    keywords = jieba.analyse.extract_tags(text, topK=20, withWeight=False, allowPOS=())\n",
    "    return keywords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "233145672587084987744302139138853522489"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "\n",
    "#hash\n",
    "def get_hash(text):\n",
    "    # int(hashlib.sha1(text.encode('utf-8')).hexdigest(), 16)\n",
    "    return int(hashlib.md5(text.encode('utf-8')).hexdigest(), 16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lines_seen=set()\n",
    "hash_index = SimhashIndex(lines_seen, k=3)\n",
    "with open('datasets/pre_text.txt',mode='r',encoding='utf-8') as f1,open('datasets/final_text.txt',mode='w',encoding='utf-8') as f2:\n",
    "    for line in f1:\n",
    "        is_duplicated=False\n",
    "        tokens = str_tokenize(line) # 分词\n",
    "        hash_index.get_near_dups(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "simhash算法示例代码（csdn上找的）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# !/usr/bin/python\n",
    "# coding=utf-8\n",
    "\n",
    "class Simhash:\n",
    "    def __init__(self, tokens='', hashbits=128):\n",
    "        self.hashbits = hashbits\n",
    "        self.hash = self.simhash(tokens)\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.hash)\n",
    "\n",
    "    # 生成simhash值\n",
    "    def simhash(self, tokens):\n",
    "        v = [0] * self.hashbits\n",
    "        for t in [self._string_hash(x) for x in tokens]:  # t为token的普通hash值\n",
    "            for i in range(self.hashbits):\n",
    "                bitmask = 1 << i\n",
    "                if t & bitmask:\n",
    "                    v[i] += 1  # 查看当前bit位是否为1,是的话将该位+1\n",
    "                else:\n",
    "                    v[i] -= 1  # 否则的话,该位-1\n",
    "        fingerprint = 0\n",
    "        for i in range(self.hashbits):\n",
    "            if v[i] >= 0:\n",
    "                fingerprint += 1 << i\n",
    "        return fingerprint  # 整个文档的fingerprint为最终各个位>=0的和\n",
    "\n",
    "    # 求海明距离\n",
    "    def hamming_distance(self, other):\n",
    "        x = (self.hash ^ other.hash) & ((1 << self.hashbits) - 1)\n",
    "        tot = 0\n",
    "        while x:\n",
    "            tot += 1\n",
    "            x &= x - 1\n",
    "        return tot\n",
    "\n",
    "    # 求相似度\n",
    "    def similarity(self, other):\n",
    "        a = float(self.hash)\n",
    "        b = float(other.hash)\n",
    "        if a > b:\n",
    "            return b / a\n",
    "        else:\n",
    "            return a / b\n",
    "\n",
    "    # 针对source生成hash值\n",
    "    def _string_hash(self, source):\n",
    "        if source == \"\":\n",
    "            return 0\n",
    "        else:\n",
    "            x = ord(source[0]) << 7\n",
    "            m = 1000003\n",
    "            mask = 2 ** self.hashbits - 1\n",
    "            for c in source:\n",
    "                x = ((x * m) ^ ord(c)) & mask\n",
    "            x ^= len(source)\n",
    "            if x == -1:\n",
    "                x = -2\n",
    "            return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def duplicating(pre_file, final_file, threshold):\n",
    "    \"\"\"去除重复新闻，即将用不同的表述方式描述同一新闻的文本视为同一条文本\"\"\"\n",
    "    lines_seen = set()  #用于存储已经出现过的行\n",
    "    with open(pre_file, mode='r', encoding='utf-8') as f1, open(final_file, mode='w', encoding='utf-8') as f2:\n",
    "        for line in f1:\n",
    "            is_duplicated = False\n",
    "            # 去除表情、数字、字母、特殊字符\n",
    "            # line = text_cleaning(remove_emoji(line))\n",
    "            line_hash = Simhash(line,16)\n",
    "            for line_seen in lines_seen:\n",
    "                seen_hash=Simhash(line_seen,16)\n",
    "                if line_hash.similarity(seen_hash) >= threshold:\n",
    "                    is_duplicated = True\n",
    "                    break\n",
    "            if not is_duplicated:\n",
    "                f2.write(line)\n",
    "                lines_seen.add(line)\n",
    "\n",
    "duplicating('datasets/pre_text.txt', 'datasets/final_text2.txt', 0.8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}